{
  "name": "Ollama",
  "logo": "QmdJujFNrnbZ8Z9SVzrZGoUEvcxg5TQqizNg51nUcwcp5Z",
  "category": [
    "ai",
    "backend"
  ],
  "tags": [],
  "port": "11434 (HTTP - API)",
  "short_description": "Local LLM inference server - run models like Llama, Mistral, and more",
  "description": "Local LLM inference server - run models like Llama, Mistral, and more",
  "usecases": [],
  "website": "https://ollama.ai",
  "dependencies": []
}
