{
  "name": "Ollama",
  "logo": "QmdJujFNrnbZ8Z9SVzrZGoUEvcxg5TQqizNg51nUcwcp5Z",
  "tags": [
    "ai",
    "backend",
    "gaming",
    "server",
    "self-hosted",
    "homelab"
  ],
  "port": "11434 (HTTP - API)",
  "short_description": "Local LLM inference server - run models like Llama, Mistral, and more",
  "description": "Local LLM inference server - run models like Llama, Mistral, and more",
  "usecases": [],
  "website": "https://ollama.ai",
  "dependencies": []
}
